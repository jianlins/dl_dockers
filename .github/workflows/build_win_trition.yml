name: build_win_triton
run-name: Build ${{ github.event.inputs.build_folder }}
on:
  workflow_dispatch:
    inputs:
      build_folder:
        description: "From which folder's environment.yml, the conda environment will be built"
        type: string   
        required: false
        default: 'win_triton'
      target_folder:
        description: 'Envs path (must be on D drive)'
        type: string
        required: false
        default: 'D:/conda_envs_jianlins/'
      download_jars:
        description: "whether download jars for pyspark"
        type: string
        required: false
        default: 'false'
      download_spacy_model:
        description: "whether download sm spacy models"
        type: string
        required: false
        default: 'true'
      zip_vol_size:
        description: 'Max 7zip volumn size'
        type: string
        required: false
        default: '400m'      
      retention_days:
        description: 'Days to keep the artifacts'
        type: int
        required: false
        default: 7

permissions:
  actions: write
  contents: read
        
jobs:
  create_env:
    runs-on: windows-latest
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
        
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.x'
    
    - name: Check Setting Folder
      run: |
        pwd
        ls ${{ github.event.inputs.build_folder }}
    
    - name: Cache Miniforge Environment
      uses: actions/cache@v4
      with:
        path: ${{ github.event.inputs.target_folder }}/${{ github.event.inputs.build_folder }}
        key: ${{ runner.os }}-${{ github.event.inputs.build_folder }}-${{ hashFiles('${{ github.event.inputs.build_folder }}/environment.yml') }}
        restore-keys: |
          ${{ runner.os }}-${{ github.event.inputs.build_folder }}-

    - name: Install miniforge
      uses: conda-incubator/setup-miniconda@v3
      with:
        auto-activate-base: true
        miniforge-version: latest
    
    - name: Create Conda Environment
      shell: pwsh
      run: |
        cd ${{ github.event.inputs.build_folder }}
        $envPath = "${{ github.event.inputs.target_folder }}/${{ github.event.inputs.build_folder }}"
        # Check if the environment directory exists
        if (Test-Path -Path $envPath) {
          Write-Host "Environment ${{ github.event.inputs.build_folder }} already exists under ${{ github.event.inputs.target_folder }}. Will try update..."
          conda env update -f environment.yml -p ${{ github.event.inputs.target_folder }}/${{ github.event.inputs.build_folder }}
        } else {
          # Environment doesn't exist, proceed to create a new one
          Write-Host "Creating new environment ${{ github.event.inputs.build_folder }} under ${{ github.event.inputs.target_folder }}."
          conda env create -f environment.yml -p ${{ github.event.inputs.target_folder }}/${{ github.event.inputs.build_folder }}
        }
        echo "clean up cache..."
        conda clean --all -f -y
        pip cache purge

    - name: check final environment settings
      run: |
        conda activate ${{ github.event.inputs.target_folder }}/${{ github.event.inputs.build_folder }}
        conda env export

    - name: Set HADOOP_HOME environment variable
      run: |
        New-Item -ItemType Directory -Force -Path "C:\hadoop\bin"
        echo "HADOOP_HOME=C:\hadoop" | Out-File -FilePath $env:GITHUB_ENV -Append
        echo "C:\hadoop\bin" | Out-File -FilePath $env:GITHUB_PATH -Append

    - name: Download Hadoop DLLs for Windows
      env:
        PYSPARK_JARS_IVY: ${{ github.event.inputs.target_folder }}/ivy
      run: |
        Invoke-WebRequest -Uri "https://github.com/steveloughran/winutils/raw/master/hadoop-3.0.0/bin/winutils.exe" -OutFile "C:\hadoop\bin\winutils.exe"
        # Assuming hadoop.dll is also required and available at a certain URL - replace this URL with the actual location for hadoop.dll
        Invoke-WebRequest -Uri "https://github.com/steveloughran/winutils/raw/master/hadoop-3.0.0/bin/hadoop.dll" -OutFile "C:\hadoop\bin\hadoop.dll"
        echo "Create .ivy folder"        
        New-Item -ItemType Directory -Force -Path  ${{ env.PYSPARK_JARS_IVY }}/jars

    # change the default .ivy location to ${{ github.event.inputs.target_folder }}/ivy so that can be zipped together easily
    - name: Cache Ivy jars
      if: ${{ github.event.inputs.download_jars == 'true' }}
      uses: actions/cache@v4
      with:
        path: |
          $ivyDir =  ${{ github.event.inputs.target_folder }}/ivy/jars
        key: ${{ runner.os }}-ivy-${{ hashFiles('**/*.jar') }}
        restore-keys: |
          ${{ runner.os }}-ivy-

    - name: download sparknlp jars
      if: ${{ github.event.inputs.download_jars == 'true' }}
      env:
        PYSPARK_JARS_IVY: ${{ github.event.inputs.target_folder }}/ivy
      run: |
        echo "PYSPARK_JARS_IVY=${{ env.PYSPARK_JARS_IVY }}"
        conda activate ${{ github.event.inputs.target_folder }}/${{ github.event.inputs.build_folder }}
        if (!(Test-Path -Path "${{ env.PYSPARK_JARS_IVY }}/jars/spark-nlp-gpu-assembly-5.2.3.jar")) {
            curl -Lo "${{ env.PYSPARK_JARS_IVY }}/jars/spark-nlp-gpu-assembly-5.2.3.jar" https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-5.2.3.jar
          } else {
            echo "JAR already exists in ${{ env.PYSPARK_JARS_IVY }}/jars"
          }
        python -c "from pyspark.sql import SparkSession; spark = SparkSession.builder.appName('Spark NLP').master('local[*]').config('spark.jars.ivy', '${{ github.event.inputs.target_folder }}/ivy').config('spark.jars.packages', 'com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:4.2.2').getOrCreate(); spark.stop();"
    
    - name: download spacy models & build triton
      if: ${{ github.event.inputs.download_spacy_model == 'true' }}
      run: |
        conda activate ${{ github.event.inputs.target_folder }}/${{ github.event.inputs.build_folder }}
        python -m spacy download en_core_web_sm
        python -m spacy download en_core_web_md
        python -m spacy download en_core_web_trf
        git clone https://github.com/triton-inference-server/server
        cd server
        python build.py -v --no-container-build --build-dir=`pwd`/build --enable-all


    # - name: copy cached jars
    #   run: |
    #     $sourceDir = Join-Path -Path $env:USERPROFILE -ChildPath ".ivy"
    #     Get-ChildItem -Path $sourceDir -Recurse -Filter *.jar | Copy-Item -Destination ${{ github.event.inputs.target_folder }}/${{ github.event.inputs.build_folder }}/lib/site-packages/pyspark/jars -Force

    - name: check folder
      run: |
        ls ${{ github.event.inputs.target_folder }}
        ls ${{ github.event.inputs.target_folder }}/${{ github.event.inputs.build_folder }}
        
    - name: check jar folder
      if: ${{ github.event.inputs.download_jars == 'true' }}
      run: |
        ls ${{ github.event.inputs.target_folder }}/ivy
        ls ${{ github.event.inputs.target_folder }}/ivy/jars
        Remove-Item -Path ${{ github.event.inputs.target_folder }}/ivy/cache/* -Recurse -Force

        
    - name: Check for 7-Zip installation
      run: |
        if (!(Test-Path "C:\Program Files\7-Zip\7z.exe")) {
          choco install 7zip
        }
  
    - name: Compress and split folder
      run: |
        pwd
        7z a -t7z -v${{ github.event.inputs.zip_vol_size }} zipped/${{ github.event.inputs.build_folder }}.7z ${{ github.event.inputs.target_folder }}*
        ls zipped

    - name: Upload compressed parts as artifacts
      uses: actions/upload-artifact@v4
      with:
        name: ${{ github.event.inputs.build_folder }}
        path: zipped/*.7z.*
        retention-days: ${{ github.event.inputs.retention_days }}